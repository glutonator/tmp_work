\newpage % Rozdziały zaczynamy od nowej strony.
\section{Eksperymenty}

% \colorbox{yellow}{todo: opisać to jakoś sensownie, punkt po punkcie co i dlaczego robię, bo teraz jest tak mega chatycznie, że nie da sie nic wywniskować}\\

% //dlaczego takie sieci został ywybran, dlaczeg otakimi paramterami steruję -> na co powinny one wpływać

%Celem pracy było stworzenie modelu pozwalającego na detekcję ironii i sarkazmu w tekście. Przy czym praca, skupia się bardziej na analizie krótkich form wypowiedzi, składających się z nie więcej niż kilku zdań. Dłuższe formy wypowiedzi o charakterze ironicznym takie jak na przykład felietony wymagają innego typu analizy i nie są uwzględniane w ramach pracy. Ponadto w ramach badań podjęta jest analiza wpływu cech morfosyntaktycznych na jakość klasyfikacji modelu. Analiza ta wynika z chęci zweryfikowaniem tezy, że dodatkowe informacje na temat roli słowa w zdaniu pozwolą na ujednoznacznienie znaczenia słowa, co przełoży się na lepszą jakość analizy tekstu w tym konkretnym zadaniu klasyfikacji.


%W ramach pracy została dokonana analiza wpływu oznaczeń morfosyntaktycznych na jakość klasyfikacji tekstów ironicznych. Na potrzeby języka angielskiego taka analiza sprowadza się głownie do analizy wpływu typu części mowy do jakiego należą poszczególne słowa i taka analiza zostanie podjęta w kolejnych krokach.

% \colorbox{yellow}{highlight}\\

Na potrzeby analizy wykorzystano wiele różnych modeli sieci neuronowych, bazujących głownie na trzech typach warstw:

\begin{itemize}
    \item gęstej;
    \item konwolucyjnej;
    \item LSTM;
\end{itemize}

W ramach różnych architektur testowany był wpływ kilku hiperparametrów na jakość klasyfikacji. Oto lista niektórych z nich:
\begin{itemize}
    \item dla warstwy gęstej:
          \begin{itemize}
              \item  1/2/3 warstwy
              \item Z dropoutem / bez dropoutu
              \item Wielkość wartwy ukrytej 30-200
          \end{itemize}

    \item dla warstwy konwolucyjnej;
          \begin{itemize}
              \item  1/2/3 warstwy
              \item Wielkość okna 3-15
              \item Z dropoutem / bez dropoutu
              \item Wielkość wartwy ukrytej 30-200
          \end{itemize}
    \item dla warstwy LSTM;
          \begin{itemize}
              \item Wykorzystanie warstwy pojedynczej (LSTM) oraz podwójnej (Bi-LSTM)
              \item  1/2/3 warstwy
              \item Z dropoutem / bez dropoutu
              \item Wielkość wartwy ukrytej 30-200
          \end{itemize}
\end{itemize}

% \subsection{POSTAGS}
% %todo:
% //todo: zmergować z dugim rozdiałem o postagach

% Aby możliwe było wykorzystanie przez sieć neuronową informacji na temat typu części mowy danego słowa konieczne są następujące kroki:

% \begin{enumerate}
%     \item Rozpoznanie części mowy w kontekście zdania i przypisanie do niego odpowiedniego tagu,
%     \item Konwersja wykrytego tagu do liczbowej reprezentacji wektorowej,
%     \item Dołączenie utworzonego wektora do przetrenowanego wektora embedingów.
% \end{enumerate}

% \hfill \break % to dodaje pustą linie po czymść dziwnym -lista, tabela
% Do rozpoznania części mowy zostało wykorzystane narzędzie z pakietu NLTK, które pozwala na oznaczenie każdego słowa w zdaniu jednym z 46 tagów, każdy odpowiadający innej części mowy. Następnie wykryty tag został przetworzony do postaci “one-hot encoding” i dodany do istniejącego embedingu słowa, zwiększając tym samy jego rozmiar o długość 46.


\subsection{Wyniki analiza}

\subsubsection{Eksperyment 1 - analiza wpływu POS tagów na jakoś klasyfikacji}
% Aby w pełni zweryfikować wpływ części mowy (POS tagów) na jakość klasyfikacji zostały wykorzystane trzy różne embeddingi oraz dwa różne zbiory danych wejściowych. Embeddingi różnią się one między innymi długością oraz metodą ich uzyskiwania. A zbiory danych różnią się licznością i stopniem zaszumienia. Każdy ze zbiorów danych został podzielony na trzy zbiory treningowy, walidacyjny oraz testowy w proporcjach 60/20/20 procent.

Aby w pełni zweryfikować wpływ części mowy (POS tagów) na jakość klasyfikacji zostały wykorzystane trzy różne embeddingi oraz dwa różne zbiory danych wejściowych. Wykorzsytane embeddingi to: Glove, FastText oraz ELMo, a ich dokładniejszy opis znajduje się w rozdziale \ref{embeddings}. Natomiast dwa zbiory danych wejściowych (oznaczonych jako A oraz B) są opisane w rozdziale \ref{dane_wejsciowe}. Na potrzeby budowania modeli każdy ze zbiorów danych został podzielony na trzy podzbiory: treningowy, walidacyjny oraz testowy w proporcjach 60/20/20 procent.

% Poniżej przedstawiono tabele z wynikami obliczeń.
Wyniki uzyskane przez modele na danych testowych zbioru A zostały umieszczone w rozdziale \ref{wyniki_eksperymentow_eks_1} i \ref{wyniki_eksperymentow_eks_1b} w tabelach \ref{tab:wyniki_glove_A}, \ref{tab:wyniki_fasttext_A}, \ref{tab:wyniki_elmo_A} , a dla zbioru B w tabelach \ref{tab:wyniki_glove_B}, \ref{tab:wyniki_fasttext_B}, \ref{tab:wyniki_elmo_B}.


W oparciu o uzyskane wyniki można stwierdzić, że wraz z dołączeniem informacji o częściach mowy nie obserwuje się widocznej poprawy jakości klasyfikacji, czasem występuje nawet jej pogorszenie. Jednym z powodów takiego zachowania może być fakt, że rodzaj części mowy nie jest kluczowy do rozpoznania ironicznego charakteru tekstu. Możne to być też spowodowane tym, że sieci wychwytują niezbędne dla nich informacje na temat roli konkretnego słowa w zdaniu w oparciu o uzyskiwany kontekst i dodatkowa informacja w ramach embeddingu jest już niepotrzebna.

Ponadto można zaważyć, że wraz ze wzrostem rozmiaru embedingów poprawia się jakość klasyfikacji, co jest zgodne z oczekiwaniami, jako że dłuższy embedding pozwala na zachowanie większej liczby informacji na temat słowa.


Analizując uśrednione pomiary jakości klasyfikacji dla sieci opartych o różne typy warstw, można zauważyć, że najgorzej sprawują się sieci oparte o warstwy gęste. Natomiast sieci wykorzystujące warstwy konwolucyjne oraz LSTM posiadają podobną skuteczność klasyfikacji. Aby wykluczyć wpływ zaszumienia danych oraz przeuczenia sieci wykonano jeszcze dwa eksperymenty mające porównać skuteczność klasyfikacji sieci opartych na warstwach konwolucyjnych oraz LSTM. Eksperymenty te są przeprowadzone na danych wejściowych pozbawionych informacji o POS tagach, gdyż z dotychczasowych pomiarów wynika, iż mają one pomijalny wpływ na jakość klasyfikacji.

%\colorbox{yellow}{todo: todo opisanie, że zbior duży to zbiór B, a mały zbiór to zbiór A}\\
%todo opisanie, że zbior duży to zbiór B, a mały zbiór to zbiór A

\subsubsection{Eksperyment 2 - Uczenie modelu na zbiorze B i testowanie jakoś klasyfikacji na zbiorze A }

W ramach tego eksperymentu zbiór danych oznaczonych jako \textbf{B} został podzielony na dwa zbiory: treningowy oraz w walidacyjny w proporcjach 80/20 procent. Natomiast zbiór oznaczony jako A został w całości wykorzystany jako zbiór testowy.

Wyniki uzyskane przez modele w ramach tego eksperymentu zostały umieszczone w rozdziale \ref{wyniki_eksperymentow_eks_2} w tabelach \ref{tab:wyniki_25_eks2}, \ref{tab:wyniki_300_eks2}, \ref{tab:wyniki_1024_eks2}.


%todo:
% \colorbox{yellow}{todo:}\\
% //Wymyślić wnioski dla teakiej sytuacji
W oparciu o uzyskane miary jakość klasyfikacji można stwierdzić, że modele wyuczone na zbiorze B źle radzą sobie z klasyfikacją rekordów ze zbioru A. Dokładność na poziomie pięćdziesięciu paru procent wskazuje, że pomimo tego, że oba zbiory pochodzą z tego samego źródła to różnią się one na tyle bardzo swoją strukturą wewnętrzną, że żadna architektura nie może sobie poradzić z prawidłową klasyfikacją. Co potwierdza powszechnie znaną prawdę dotyczącą sieci neuronowych, że takie modele do procesu trenowania potrzebują bardzo dużej ilości danych o możliwie jak najbardziej zróżnicowanej strukturze. Wystarczy, że zbiór danych treningowych jest tylko pewnym wycinkiem dziedziny problemu, który sieć ma rozwiązać, by nauczyła się ona klasyfikacji złych cech.


\subsubsection{Eksperyment 3 - Uczenie i testowanie modelu na fragmencie zbioru B tak by był równoliczny ze zbiorem A}

W ramach tego eksperymentu ze zbioru danych oznaczonych jako \textbf{B} zostało wybranych w losowy sposób 2000 rekordów ironicznych i tyle samo nieironicznych, tak by licznością były porównywalne ze zbiorem oznaczonym jako \textbf{A}. Tak uzyskany zbiór został podzielony na trzy podzbiory: treningowy, walidacyjny oraz testowy w proporcjach 60/20/20 procent.

Wyniki uzyskane przez modele w ramach tego eksperymentu zostały umieszczone w rozdziale \ref{wyniki_eksperymentow_eks_3} w tabelach \ref{tab:wyniki_25_eks3}, \ref{tab:wyniki_300_eks3}, \ref{tab:wyniki_1024_eks3}.



W oparciu o uzyskane wyniki można zauważyć:

\begin{itemize}
    \item Wyższą średnią jakoś klasyfikacji w porównaniu do zbioru oznaczonego jako \textbf{A}
    \item Lepszą jakość klasyfikacji dla sieci opartych o warstwy typu LSTM
\end{itemize}


Wyższa jakoś klasyfikacji może wskazywać na to, że zbiór oznaczony jako \textbf{A} posiada dużo bardziej zróżnicowane dane, przy czym zbiór posiada zbyt małą liczbą próbek by sieć, w procesie nauki, odpowiednio je wychwyciła. Problem ten może być najprawdopodobniej wyeliminowany poprzez dostarczenie większej liczby próbek danych o zróżnicowanym charakterze.

Lepsza jakość klasyfikacji przez sieci oparte o warstwy LSTM jest zgodny z wnioskami płynącymi z innych publikacji badających podobne zagadnienia. Trudności z obserwacji tej zależności w poprzednich eksperymentach mogła wynikać między innymi z:


\begin{itemize}

    \item Dla zbioru A:
          \begin{itemize}
              \item Małej liczby próbek
              \item Dużej różnorodności prezentowanej ironii w próbkach
          \end{itemize}

    \item Dla zbioru B:
          \begin{itemize}
              \item Małej różnorodności prezentowanej ironii w próbkach
              \item Na tyle duża liczba próbek, by sieci o teoretycznie gorszych zdolnościach do rozwiązywania danego zadania klasyfikacji mogły wychwycić cechy tekstu pozwalające, przy odpowiednio długim procesie uczenia, na poprawną klasyfikację.
          \end{itemize}

\end{itemize}








