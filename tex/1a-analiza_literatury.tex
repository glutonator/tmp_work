% \newpage % Rozdziały zaczynamy od nowej strony.
\subsection{Stan badań}

Zagadnienie klasyfikacji ironii jest dość popularnym problemem wśród badaczy zajmujących się analizą języka naturalnego ze względu na konieczność wychwycenia nie tylko dosłownego znaczenia danej wypowiedzi, ale także detekcję i analizy ewentualnego podtekstu.

% //analiza sentymentu = analiza wydzwięku emeocjonalnego

Pierwsze prace z tej dziedziny opierały się na analizie sentymentu poszczególnych słów. Opierały się na przeświadczeniu, że jeśli wypowiedź utrzymuje stały charakter emocjonalny w postaci pozytywnego lub negatywnego wydźwięku    nie zawiera ono w sobie ironii. Natomiast jeśli pojawiały się fragmenty o przeciwnej polaryzacji, to zdanie z dużym prawdopodobieństwem można było klasyfikować jako ironiczne. Takie metody skupiały się przede wszsytkim na kryteriach ilościowych, nie uwględniając większej istotności słów kluczowych. Z tego powodu metody tego typu były dość zawodne.

% Metody te jednak były dość zawodne ze względu na brak uwzględniania znaczenia poszczególnych słów, pozwalające na …//todo

% //Odnieśc się ->  Sarcasm as Contrast between a Positive Sentiment and Negative Situation

% //todo: opisać systemy regułowe


Inne podejście, zaprezentowane w pracy \cite{Riloff2013} opierało się na spostrzeżeniach, że w zdaniach sarkastycznych słowa o pozytywnym wydźwięku kontrastują z sytuacjami nacechowanymi negatywnie. W oparciu o tą tezę badacze zaproponowali algorytm systematycznie uczący się dwóch grup sformułowań:

\begin{itemize}
    \item słów o pozytywnym wydźwięku (oznaczony jako PS)
    \item sformułowań świadczących o sytuacjach nacechowanych negatywnie (oznaczony jako NS)
\end{itemize}


\noindent Algorytm rozpoczyna swoje działanie od tylko jednego słowa ‘love’ w zbiorze słów PS. Następnie dokonywane są kolejne kroki:
\begin{enumerate}
    \item Wyszukiwanie w zbiorze danych rekordów zawierających słowo ze zbioru PS
    \item Dla każdego znalezionego rekordu w otoczeniu słowa ze zbioru PS wyznaczany jest 1-gram,2-gram oraz 3-gram.
    \item Ze stworzonych n-gramow wybierany jest ten, który występuje najczęściej dla rekordów sarkastycznych w otoczeniu słów ze zbioru PS i jest dodawany do zbioru NS
    \item Następnie dokonywane jest wyszukiwanie w zbiorze danych, takich rekordów, które zawierają słowa ze zbioru NS
    \item Dla każdego znalezionego rekordu w otoczeniu słowa ze zbioru NS wyznaczany jest 1-gram,2-gram oraz 3-gram.
    \item Ze stworzonych n-gramow wybierany jest ten, który występuje najczęściej dla rekordów sarkastycznych w otoczeniu słów ze zbioru NS i jest dodawany do zbioru PS
    \item Powrót do kroku 1, aż do momentu, gdy zbiory PS i NS będą wystarczająco liczne.
\end{enumerate}

\hfill 


\noindent Dzięki takiemu podejście możliwe jest zidentyfikowanie najczęściej pojawiających się kombinacji wskazujących na występowanie sarkazmu i w oparciu o nie dokonać predykcji czy dana wypowiedź ma charakter sarkastyczny. Istotną wadą takiego podejścia jest dość wąskie okno w ramach dokonywana jest analiza zdania, a także brak uwzględniania kulturowego znaczenia poszczególnych słów.

Inne podejśćie reprezentuje praca \cite{one_more_source_to_lit}. Bada ona wpływ różnych operacji dokonanych podczas wstępnej obróbki danych na jakość klasyfikacji tekstów ironicznych i sarkastycznych. Badacze wykorzystali w trakcie pracy dwa rodzaje metod pozwalających na utworzenie reprezentacji wektorowej słów i znaków, oba z nich bazują na zliczaniu współwystępowania słów w zbiorze danych. Do zadania klasyfikacji został wykorzystany między innym algorytm K najbliższych sąsiadów oraz SVM. Badacze uzyskali wartość metryki F1 na poziomie około 60 procent, co wskazuje na duże niedoskonałości zaproponowanych metod i skłania do poszukiwania innych, bardziej efektywnych algorytmów. 

Ze względu na obserwowaną lepszą skuteczność klasyfikacji, późniejsze prace skupiały się przede wszystkim na wykorzystaniu sieci neuronowych do zagadnienia rozpoznawania ironii.

Kluczowym elementem w przypadku wykorzystania sieci neuronowych jest wstępne przetworzenie danych. Dlatego prace \cite{Baziotis2018}  \cite{Huang2017}  \cite{Ilic2018} z tej dziedziny skupiały się w pierwszej kolejności na usunięciu lub zastąpieniu specjalnymi oznaczeniami takich informacji jak linki do zewnętrznych portali oraz oznaczenie użytkownika. Ponadto tekst często był normalizowany poprzez wykorzystanie lematyzacji i stemmingu co miało na celu zmniejszenie liczby unikalnych słów. 

Kolejnym istotnym elementem w ramach przetwarzania tekstu, wykorzystującego sieci neuronowe, był sposób konwersji słów do przestrzeni liczbowej. W publikacjach autorzy wykorzystywali różne metody, między innymi powszechnie znany word2vec, czy też zyskujący coraz większa popularność ELMo. Tak przetworzone dane były wprowadzane do sieci zarówno opartych o warstwy konwolucyjne, jak i warstwy typu LSTM. Jakość klasyfikacji dla różnych architekrur opartych o te warstwy była dość podobna z nieznaczną przewagą dla sieci opartych na warstwach LSTM.



